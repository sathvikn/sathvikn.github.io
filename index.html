<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Sathvik Nair</title>
  
  <meta name="author" content="Sathvik Nair">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Sathvik Nair</name>
              </p>
              <p>I'm a PhD student in <a href="https://linguistics.umd.edu/">Linguistics</a> at the University of Maryland, advised by Profs. <a href="http://users.umiacs.umd.edu/~resnik/">Philip Resnik</a> and <a href="https://www.colinphillips.net/">Colin Phillips</a>. 
              I work on computational psycholinguistics and am affiliated with the <a href = 'https://wiki.umiacs.umd.edu/clip/index.php/Main_Page'>Computational Linguistics and Information Processing (CLIP) Lab</a>, and am part of the broader <a href="https://languagescience.umd.edu/">language science community</a> at UMD. More on my collaborators below!</p>
              <p>Originally from the Bay Area, I graduated from UC Berkeley with bachelor's degrees in Cognitive Science and Computer Science, where I received the <a href="https://cogsci.berkeley.edu/major-program/honors-program/robert-j-glushko-prize-distinguished-undergraduate-research"
                >Glushko Prize for Outstanding Undergraduate Research in Cognitive Sciences</a>. There, I closely collaborated with <a href="https://stephanmeylan.com">Dr. Stephan Meylan</a> on projects in Profs. <a href="https://psychology.berkeley.edu/people/mahesh-srinivasan">Mahesh Srinivasan</a> and <a href="https://cocosci.princeton.edu/tom/index.php">Tom Griffiths'</a> groups.
                Afterwards, I worked as a software engineer at Amazon Web Services in Boston. I generally accept he/him pronouns in English. In languages with gender agreement, masculine and gender neutral forms are fine with me.</p>
              <p style="text-align:center">
                <a href="mailto:sathvik@umd.edu">Email</a> &nbsp|&nbsp
                <a href="https://twitter.com/sathvikn4">Twitter</a> &nbsp|&nbsp
                <a href="https://github.com/sathvikn">GitHub</a> &nbsp|&nbsp
                <a href="https://linkedin.com/in/sathvik-nair">LinkedIn</a> &nbsp|&nbsp
                <a href="https://www.semanticscholar.org/author/Sathvik-Nair/2003578119">Semantic Scholar</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a><img style="width:100%;max-width:100%" alt="profile photo" src="img/self.jpeg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                I focus on computational models of prediction in sentence processing. I'm interested in the interaction between language-specific information and domain-general cognitive processes like memory.
                Recently, I've been looking at ways in which experimental data from humans compares with predictions made by large language models, in the service of developing more cognitively plausible models of language processing.
                I'm also interested how cognitive modeling can interact with other NLP applications, such as interpetability and computational social sciences. 
                In general, I'm excited about various topics at the intersection of language, computation, and cognition, and am always happy to connect regarding either of these interests.
              </p>
              </td>
            </tr>
            </tbody>
          </table>
              <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                <h3>Publications</h3>
                <tr>
                  <td style="padding:20px;width:25%;vertical-align:middle">
                      <img src='img/tokenization.png' width="300">
                    </div>
                  </td>
                  <td style="padding:20px;width:75%;vertical-align:middle">
                      <papertitle>Words, Subwords, and Morphemes: What Really Matters in the Surprisal-Reading Time Relationship?
                      </papertitle>
                    <br>
                    <strong>Sathvik Nair</strong>,
                    <a href="http://users.umiacs.umd.edu/~resnik/">Philip Resnik</a>
                    <br>
                    <em>Findings of EMNLP 2023</em>
                    <br>
                    <a href="https://aclanthology.org/2023.findings-emnlp.752/">Proceedings Paper</a>
                    <p>
                    </p>
                    <p>An important assumption that comes with using LLMs on psycholinguistic data has gone unverified. LLM-based predictions are based on subword tokenization, not decomposition of words into morphemes. 
                      Does that matter? We carefully test this by comparing surprisal estimates using orthographic, morphological, and BPE tokenization against reading time data. 
                      Our results replicate previous findings. In the aggregate, predictions using BPE tokenization do not suffer relative to morphological and orthographic segmentation.
                      Finer-grained analyses show differences between BPE and morphological surprisal, suggesting that the effects of subword tokenization should be taken seriously in cognitive research, and provide promising evidence for using morphological surprisal in further research.
                    </p>
                  </td>
                </tr> 

                <tr>
                  <td style="padding:20px;width:25%;vertical-align:middle">
                      <img src='img/substitution.png' width="300">
                    </div>
                  </td>
                  <td style="padding:20px;width:75%;vertical-align:middle">
                      <papertitle>How far does probability take us when measuring psycholinguistic fit? Evidence from substitution illusions and speeded cloze data
                      </papertitle>
                    <br>
                    <strong>Sathvik Nair</strong>,
                    <a href="https://sbhattasali.github.io/">Shohini Bhattasali</a>,
                    <a>Philip Resnik</a>
                    <a href="https://colinphillips.net/">Colin Phillips</a>
                    <br>
                    <em>Poster Presentation at the 36th Annual Conference on Human Sentence Processing, 2023</em>
                    <br>
                    <a href="assets/hsp_23_abstract.pdf">Abstract</a> |
                    <a href="https://osf.io/abv7d">Poster</a>
                    <p>
                    </p>
                    <p>When asked <i>How many animals of each kind did Moses take on the ark?</i>, humans often respond with <i>two</i> instead of detecting issues with the question.
                      This effect is known as a <i>substitution illusion</i> (canonically a Moses illusion, but we refer to them this way to emphasize how they can apply in various contexts).
                      We used measures from NLP techniques on experimental data collected by <a href='http://www.colinphillips.net/wp-content/uploads/2022/08/muller2022-1.pdf'>Muller (2022)</a> to address whether people's selectivity to the illusion is better explained by semantic similarity of the correct word and substituted word (distributional vector-space representations), or the substituted word's relationship to the context(masked language modeling).
                      On a separate speeded cloze task (<a href='https://www.sciencedirect.com/science/article/pii/S0749596X15000236'>Staub et al, 2015</a>), we found that language model probability corresponded strongly with cloze probability, but not with reaction time, which is seen as a stronger measure of lexical predictability in context.
                      This work reveals some limitations of applying language models to psycholinguistic data.
                    </p>
                  </td>
                </tr> 

                  <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <img src='img/table_tsne.png' width="300">
                      </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <papertitle>Contextualized Word Embeddings Capture Human-Like Relations Between English Word Senses</papertitle>
                      <br>
                      <strong>Sathvik Nair</strong>,
                      <a href="https://lcdlab.berkeley.edu/people/">Mahesh Srinivasan</a>,
                      <a href="https://stephanmeylan.com/">Stephan Meylan</a>
                      <br>
                      <em>Oral Presentation for Cognitive Aspects of the Lexicon workshop(CogALex VI) at International Conference on Computational Linguistics (COLING)</em>, 2020  
                      <br>
                      <em>Undergraduate Honors Thesis, advised by Dr. Meylan, Prof. Srinivasan, and Prof. <a href = 'http://colala.berkeley.edu/people/piantadosi/'>Steven Piantadosi</a>, received the <a href="https://cogsci.berkeley.edu/major-program/honors-program/robert-j-glushko-prize-distinguished-undergraduate-research"
                >Glushko Prize for Outstanding Undergraduate Research in Cognitive Sciences</a>.</em>
                      <br>
                      <a href="https://www.aclweb.org/anthology/2020.cogalex-1.16/">Paper</a> |
                      <a href="assets/cogsci_thesis.pdf">Thesis</a> |
                      <a href="https://osf.io/fm78w/">Code and Data</a>
                      <p></p>
                      <p>We investigated whether recent advances in NLP (specifically the Transformer-based neural network model BERT), are able to capture human-like distinctions between 
                        meanings of the same word, such as polysemy and homonymy. We collected human judgements of the relatedness of selected WordNet senses for 32 English words 
                        from a two-dimensional spatial arrangement task, and compare them with relatedness according to BERT vectors for these corresponding senses in the SemCor corpus.
                        We demonstrated participants’ judgments of the relatedness between senses are correlated with distances between senses in the BERT embedding space, and that BERT encodes
                        homonymous sense relations closer to human judgements than polysemous ones. 
                  </p>
                    </td>
                  </tr> 
                  <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <img src='img/telephone.png' width="300">
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <papertitle>Evaluating Models of Robust Word Recognition with Serial Reproduction.</papertitle>
                      <br>
                      <a>Stephan Meylan</a>,
                      <strong>Sathvik Nair</strong>,
                      <a href="https://cocosci.princeton.edu/tom/tom.php/">Tom Griffiths</a>
                      <br>
                      <em>Published in May 2021 issue of <i>Cognition</i> journal</em>
                      <br>
                      <a href="https://www.sciencedirect.com/science/article/abs/pii/S0010027720303723">Journal Article</a> |
                      <a href="https://arxiv.org/abs/2101.09788">Preprint (full text)</a>
                      <br>
                      <p>We compared how several probabilistic generative language models, such as n-grams, probabilistic context free grammars (PCFGs), and neural networks, 
                         capture human linguistic expectations in a web-based serial reproduction task, in which in which participants try to repeat sentences said by other participants,
                         similar to a game of "Telephone." We found that models that make use of preceding context, especially those with abstract representations of linguistic structure, best predict changes participants
                         made when trying to reproduce utterances in the experiment. I contributed to designing and implementing parts of the experimental interface, 
                         extracting probabilities under PCFGs, modeling which words in utterances were most likely to change under the models, and revising the final paper.
                      </p>
                    </td>
                  </tr> 
        </tbody></table>

        <h3>Collaborators, Mentors, Friends, and other Co-Conspirators</h3>
        <p>Research is never done in a vacuum, and publications don't reflect everyone who's intellectually influenced me. Here are some of those people. </p>
          <ul>
              <li>UMD Cohort: <a href='https://smancha.github.io/'><b>S</b>ebastián Mancha</a>, <a href="https://utkuturk.com/"><b>U</b>tku Türk</a>, <a href="https://linguistics.umd.edu/directory/sarah-boukendour"><b>S</b>arah Boukendour</a>, 
                <a href="https://linguistics.umd.edu/directory/cassandra-caragine"><b>C</b>assandra Caragine</a>, <a href="https://lydiayquevedo.wordpress.com/"><b>L</b>ydia Quevedo</a>, <a href="https://linguistics.umd.edu/directory/allison-dods"><b>A</b>llison Dods</a>,
                <a href="https://www.malshah.com/"><b>M</b>alhaar Shah</a>, <b>S</b>athvik Nair (SUS CLAMS for short)
              </li>
              <li>Labmates: <a href="https://styx97.github.io/">Rupak Sarkar</a>, <a href="https://alexanderhoyle.com/">Alexander Hoyle</a>, <a href="https://pranav-goel.github.io/">Dr. Pranav Goel</a>, <a href="https://linguistics.umd.edu/directory/london-dixon">London Dixon</a>,
                <a href="https://katherinehowitt.com/">Katherine Howitt</a>, <a href="https://ekrosalee.wordpress.com/">Rosa Lee</a>, <a href="https://sites.google.com/view/masato-nakamura/">Dr. Masato Nakamura</a>,  Allison MacDonald, 
                <a href="https://joselynrodriguez.com/">Joselyn Rodriguez</a>, <a href="https://n-ika.github.io/">Nika Jurov</a>, <a href="https://www.craigthorburn.org/">Dr. Craig Thorburn</a>, <a href="https://nehasrikn.github.io/">Neha Srikanth</a>, <a href="https://mgor.info/">Maharshi Gor</a>, <a href="https://houyu0930.github.io/">Yu (Hope) Hou</a>
              </li>
              <li>Collaborators and Mentors: <a href="https://jmankewitz.com/">Jess Mankewitz</a>, <a href="https://www.sammyfloyd.com/">Dr. Sammy Floyd</a>, <a href="https://mikabr.io/">Dr. Mika Braginsky</a>, <a href="http://users.umiacs.umd.edu/~nhf/">Prof. Naomi Feldman</a>, <a href="https://ellenlau.net/">Prof. Ellen Lau</a>,
              <a href="https://sbhattasali.github.io/">Prof. Shohini Bhattasali</a>, <a href="https://stephanmeylan.com/">Dr. Stephan Meylan</a>, <a href="https://ruthefoushee.com/">Dr. Ruthe Foushee</a>, <a href="https://www.acsu.buffalo.edu/~cxjacobs/">Prof. Cassandra Jacobs</a></li>
          </ul>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Teaching</heading>
            <p>At UMD:
              <ul>
                <li><a href = "https://app.testudo.umd.edu/soc/search?courseId=LING449D&sectionId=&termId=202308&_openSectionsOnly=on&creditCompare=%3E%3D&credits=0.0&courseLevelFilter=ALL&instructor=&_facetoface=on&_blended=on&_online=on&courseStartCompare=&courseStartHour=&courseStartMin=&courseStartAM=&courseEndHour=&courseEndMin=&courseEndAM=&teachingCenter=ALL&_classDay1=on&_classDay2=on&_classDay3=on&_classDay4=on&_classDay5=on">LING499D-Undergraduate Seminar in Psycholinguistics</a>: Guest Lecturer in Fall 2023</li>
                <li><a href = "https://app.testudo.umd.edu/soc/search?courseId=LING200&sectionId=&termId=202308&_openSectionsOnly=on&creditCompare=&credits=&courseLevelFilter=ALL&instructor=&_facetoface=on&_blended=on&_online=on&courseStartCompare=&courseStartHour=&courseStartMin=&courseStartAM=&courseEndHour=&courseEndMin=&courseEndAM=&teachingCenter=ALL&_classDay1=on&_classDay2=on&_classDay3=on&_classDay4=on&_classDay5=on">LING200-Introduction to Linguistics</a>: Teaching Assistant in Fall 2023</li>
              </ul>
            </p>
            <p>At UC Berkeley:
              <ul>
                <li><a href = "https://classes.berkeley.edu/content/2020-spring-cogsci-131-001-lec-001">COGSCI 131-Computational Models of Cognition</a>: Undergraduate student instructor 
                  in Spring 2020</li>
                <li><a href = "http://data8.org/">DATA 8-Foundations of Data Science</a>: Undergraduate student instructor from Spring-Fall 2019, tutor from Spring-Fall 2018.</li>
              </ul>
            </p>
          </td>
        </tr>
      </tbody></table>

      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
        <td style="padding:20px;width:100%;vertical-align:middle">
          <heading>Miscellaneous</heading>
          <p>Other projects (not just academic) and information.
            <ul>
              <li><a href="https://sathvikn.notion.site/Large-Language-Models-Levels-of-Analysis-What-40-year-old-Neuroscience-Research-can-Tell-Us-About-91bcbd39fd3d47af916c73214d17103b">Large Language Models & Levels of Analysis: What 40-year-old Neuroscience Research can Tell Us About Modern AI
              </a>– Applying fundamental ideas from cognitive science (Marr's levels of analysis) to explain various kinds of language models and argue how we can't say LLMs "understand" language like humans do.
              <li><a href="https://langcog.github.io/childes-db-website/">childes-db</a> - I helped update & refactor the implementation of a relational database interface to CHILDES, a collection of multilingual child language corpora, so it can be easily accessed in Python and R.</li> 
              <li><a href="https://www.skynettoday.com/briefs/gpt3">GPT-3: An AI Breakthrough, but not Coming for Your Job
              </a>– Article describing GPT-3, reactions from the press and experts, and research-backed opinions on the technology's limitations for Skynet Today (AI news publication). Coauthored with <a href = "https://db7894.github.io/">Daniel Bashir</a></li>
              <li><a href ='https://towardsdatascience.com/how-biases-in-language-get-perpetuated-by-technology-b4edc5532f3f'>How Biases in Language get Perpetuated by Technology
              </a>– Towards Data Science article on personal project investigating gender, racial, and religious bias through analogy evaluation with static word embeddings (GloVe)</li>
              <li><a href = "https://github.com/sathvikn/spectra-nlp-workshop">Workshop on NLP/ML</a>– given at <a href = "https://www.facebook.com/events/make-school/spectra-30-hackathon/2334056506615102/"> the Spectra 3.0 hackathon.</a>
              Presented overview of the field and sentiment classification demo on tweets related to mental health.</li>
              <li><a href ="https://lettersforblacklives.com/south-asian-us-edition-d2819da03774">Letters for Black Lives</a>- I was involved with writing & curating resources for the South Asian community on anti-Blackness, including Hindi translation.</li>
              <li>Here are some organizations whose work I care about: <a href='https://dmvmutualaid.carrd.co/'>DMV Mutual Aid</a>, <a href="https://kalamamutualaid.org/">Kalama Mutual Aid</a>, <a href="https://www.queerinai.com/">Queer in AI</a>, <a href="https://www.solidaritysummer.org/">Bay Area Solidarity Summer</a>, <a href='https://sogoreate-landtrust.org/'>Sogorea Te' Land Trust</a></li>
              <li>In my spare time, I enjoy playing violin & South Asian percussion, cooking, going on runs and hikes, and most recently, exploring the East Coast.</li>
            </ul>
          </p>
        </td>
      </tr>
    </tbody></table>

<span style= "text-align: center"><a href = "https://github.com/jonbarron/website">Website Template</a></span>
</body>

</html>
